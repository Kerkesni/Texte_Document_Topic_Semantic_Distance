{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MrZanziba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MrZanziba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Loading Data...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Used Ressources :\n",
    "- https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel\n",
    "- https://www.kaggle.com/ragnisah/text-data-cleaning-tweets-analysis\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import ast\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "os.environ.update({'MALLET_HOME':os.path.join(os.getcwd(), 'other\\mallet-2.0.8')})\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Data Loading\n",
    "def load_data(url):\n",
    "    data = pd.read_csv(url,)\n",
    "    return data\n",
    "\n",
    "# Punctuation removal\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in str(text) if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "# Sub division of strings into arrays of words\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "# Stop words removal \n",
    "def remove_stopwords(text):\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "# Getting the root/base of words\n",
    "def stemming(text):\n",
    "    ps = nltk.PorterStemmer()\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "# Linking similar words to one word\n",
    "def lemmatizer(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "# Text processing\n",
    "def process_dataframe(data):\n",
    "    df  = pd.DataFrame(data[['text']])\n",
    "    df['text_punct'] = df['text'].apply(lambda x: remove_punct(x))\n",
    "    df['text_tokenized'] = df['text_punct'].apply(lambda x: tokenization(x.lower()))\n",
    "    df['text_nonstop'] = df['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "    df['text_stemmed'] = df['text_nonstop'].apply(lambda x: stemming(x))\n",
    "    df['text_lemmatized'] = df['text_nonstop'].apply(lambda x: lemmatizer(x))\n",
    "    df = df.drop(columns=['text_punct', 'text_tokenized', 'text_nonstop', 'text_stemmed'])\n",
    "    df = df.rename(columns={'text':'original_text', 'text_lemmatized':'processed_text'})\n",
    "    return df\n",
    "\n",
    "# Get Dominant Topic for text\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "# Getting Quality Measures of the LDA Model\n",
    "def getPerplexityCoherence(model, corpus, texts, dictionary):\n",
    "    perplexity = model.log_perplexity(corpus)\n",
    "    coherence_model_lda = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence = coherence_model_lda.get_coherence()\n",
    "    return perplexity,coherence\n",
    "\n",
    "# Getting dominant topic for each text\n",
    "def getDominantTopicsForTexts(ldaModel, corpus, texts):\n",
    "    print('Getting Dominant Topics...')\n",
    "    df_topic_sents_keywords = format_topics_sentences(ldaModel=ldaModel, corpus=corpus, texts=texts)\n",
    "    #Formating The Results\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    return df_dominant_topic\n",
    "\n",
    "# Getting Array of Topics\n",
    "def getTopics(ldaModel, n_topics):\n",
    "    print(\"Retreiving Topics...\")\n",
    "    topics = []\n",
    "    for index in range(20):\n",
    "        topic = []\n",
    "        for word, n in topic_modeler.show_topic(index):\n",
    "            topic.append(word)\n",
    "        topics.append(topic)\n",
    "    return topics\n",
    "\n",
    "# Training Or loading topic modeler\n",
    "def getTopicExtractor(corpus, dictionary, load=True):\n",
    "    if load:\n",
    "        # Loading Mallet LDA Model\n",
    "        print('Loading LDA Mallet Model...')\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet.load(\"model/mallet_model/mallet\")\n",
    "        return ldamallet\n",
    "    else:\n",
    "        # Training Mallet LDA Model\n",
    "        print('Training LDA Mallet Model')\n",
    "        mallet_path = os.path.join(os.getcwd(), 'other\\\\mallet-2.0.8\\\\bin\\\\mallet')\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow_corpus, num_topics=20, id2word=dictionary)\n",
    "        # Saving The Model\n",
    "        ldamallet.save(\"model/mallet_model/mallet\")\n",
    "        return ldamallet\n",
    "\n",
    "# Training Doc2Vec Model\n",
    "def trainDoc2VecModel(texts, load=True):\n",
    "    if load:\n",
    "        print('Loading Doc2Vec Model...')\n",
    "        model = Doc2Vec.load(\"model/doc2vec_model/doc2vec\")\n",
    "        return model\n",
    "    else:\n",
    "        print('Training Doc2Vec Model...')\n",
    "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(texts)]\n",
    "        model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "        model.save(\"model/doc2vec_model/doc2vec\")\n",
    "        return model\n",
    "\n",
    "# Loading dataset\n",
    "print('Loading Data...')\n",
    "news = load_data('./data/20_newsgroup_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pre-processing Data...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           original_text  \\\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...   \n",
       "1      From: CPKJP@vm.cc.latech.edu (Kevin Parker)\\nS...   \n",
       "2      From: jimf@centerline.com (Jim Frost)\\nSubject...   \n",
       "3      From: eliot@lanmola.engr.washington.edu (eliot...   \n",
       "4      From: sjp@hpuerca.atl.hp.com (Steve Phillips)\\...   \n",
       "...                                                  ...   \n",
       "11309  From: koontzd@phobos.lrmsc.loral.com (David Ko...   \n",
       "11310  From: schinagl@fstgds15.tu-graz.ac.at (Hermann...   \n",
       "11311  From: brad@clarinet.com (Brad Templeton)\\nSubj...   \n",
       "11312  From: amolitor@nmsu.edu (Andrew Molitor)\\nSubj...   \n",
       "11313  From: rdippold@qualcomm.com (Ron \"Asbestos\" Di...   \n",
       "\n",
       "                                          processed_text  \n",
       "0      [lerxstwamumdedu, wheres, thing, subject, car,...  \n",
       "1      [cpkjpvmcclatechedu, kevin, parker, subject, i...  \n",
       "2      [jimfcenterlinecom, jim, frost, subject, car, ...  \n",
       "3      [eliotlanmolaengrwashingtonedu, eliot, subject...  \n",
       "4      [sjphpuercaatlhpcom, steve, phillips, subject,...  \n",
       "...                                                  ...  \n",
       "11309  [koontzdphoboslrmscloralcom, david, koontz, su...  \n",
       "11310  [schinaglfstgdstugrazacat, hermann, schinagl, ...  \n",
       "11311  [bradclarinetcom, brad, templeton, subject, kn...  \n",
       "11312  [amolitornmsuedu, andrew, molitor, subject, ta...  \n",
       "11313  [rdippoldqualcommcom, ron, asbestos, dippold, ...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_text</th>\n      <th>processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n      <td>[lerxstwamumdedu, wheres, thing, subject, car,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>From: CPKJP@vm.cc.latech.edu (Kevin Parker)\\nS...</td>\n      <td>[cpkjpvmcclatechedu, kevin, parker, subject, i...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>From: jimf@centerline.com (Jim Frost)\\nSubject...</td>\n      <td>[jimfcenterlinecom, jim, frost, subject, car, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>From: eliot@lanmola.engr.washington.edu (eliot...</td>\n      <td>[eliotlanmolaengrwashingtonedu, eliot, subject...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>From: sjp@hpuerca.atl.hp.com (Steve Phillips)\\...</td>\n      <td>[sjphpuercaatlhpcom, steve, phillips, subject,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11309</th>\n      <td>From: koontzd@phobos.lrmsc.loral.com (David Ko...</td>\n      <td>[koontzdphoboslrmscloralcom, david, koontz, su...</td>\n    </tr>\n    <tr>\n      <th>11310</th>\n      <td>From: schinagl@fstgds15.tu-graz.ac.at (Hermann...</td>\n      <td>[schinaglfstgdstugrazacat, hermann, schinagl, ...</td>\n    </tr>\n    <tr>\n      <th>11311</th>\n      <td>From: brad@clarinet.com (Brad Templeton)\\nSubj...</td>\n      <td>[bradclarinetcom, brad, templeton, subject, kn...</td>\n    </tr>\n    <tr>\n      <th>11312</th>\n      <td>From: amolitor@nmsu.edu (Andrew Molitor)\\nSubj...</td>\n      <td>[amolitornmsuedu, andrew, molitor, subject, ta...</td>\n    </tr>\n    <tr>\n      <th>11313</th>\n      <td>From: rdippold@qualcomm.com (Ron \"Asbestos\" Di...</td>\n      <td>[rdippoldqualcommcom, ron, asbestos, dippold, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11314 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Processing dataset\n",
    "print('Pre-processing Data...')\n",
    "processed_df = process_dataframe(news)\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating Dictionnary...\n",
      "Creating bag of words...\n",
      "Loading LDA Mallet Model...\n",
      "Retreiving Topics...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['space',\n",
       "  'nasa',\n",
       "  'year',\n",
       "  'cost',\n",
       "  'center',\n",
       "  'program',\n",
       "  'technology',\n",
       "  'research',\n",
       "  'earth',\n",
       "  'launch'],\n",
       " ['gun',\n",
       "  'state',\n",
       "  'people',\n",
       "  'mr',\n",
       "  'law',\n",
       "  'government',\n",
       "  'president',\n",
       "  'weapon',\n",
       "  'crime',\n",
       "  'bill'],\n",
       " ['university',\n",
       "  'nntppostinghost',\n",
       "  'article',\n",
       "  'computer',\n",
       "  'replyto',\n",
       "  'science',\n",
       "  'apr',\n",
       "  'usa',\n",
       "  'dept',\n",
       "  'michael'],\n",
       " ['book',\n",
       "  'time',\n",
       "  'part',\n",
       "  'point',\n",
       "  'number',\n",
       "  'find',\n",
       "  'order',\n",
       "  'reference',\n",
       "  'read',\n",
       "  'good'],\n",
       " ['game',\n",
       "  'team',\n",
       "  'year',\n",
       "  'player',\n",
       "  'play',\n",
       "  'win',\n",
       "  'season',\n",
       "  'hockey',\n",
       "  'league',\n",
       "  'fan'],\n",
       " ['armenian',\n",
       "  'people',\n",
       "  'turkish',\n",
       "  'told',\n",
       "  'woman',\n",
       "  'greek',\n",
       "  'home',\n",
       "  'year',\n",
       "  'time',\n",
       "  'child'],\n",
       " ['dont',\n",
       "  'im',\n",
       "  'good',\n",
       "  'thing',\n",
       "  'time',\n",
       "  'make',\n",
       "  'ive',\n",
       "  'lot',\n",
       "  'youre',\n",
       "  'back'],\n",
       " ['key',\n",
       "  'chip',\n",
       "  'encryption',\n",
       "  'system',\n",
       "  'clipper',\n",
       "  'government',\n",
       "  'de',\n",
       "  'technology',\n",
       "  'phone',\n",
       "  'bit'],\n",
       " ['article',\n",
       "  'david',\n",
       "  'nntppostinghost',\n",
       "  'jim',\n",
       "  'day',\n",
       "  'im',\n",
       "  'distribution',\n",
       "  'university',\n",
       "  'fire',\n",
       "  'news'],\n",
       " ['file',\n",
       "  'window',\n",
       "  'program',\n",
       "  'image',\n",
       "  'version',\n",
       "  'application',\n",
       "  'server',\n",
       "  'display',\n",
       "  'set',\n",
       "  'code'],\n",
       " ['israel',\n",
       "  'israeli',\n",
       "  'jew',\n",
       "  'war',\n",
       "  'state',\n",
       "  'people',\n",
       "  'country',\n",
       "  'article',\n",
       "  'arab',\n",
       "  'world'],\n",
       " ['car',\n",
       "  'article',\n",
       "  'bike',\n",
       "  'dod',\n",
       "  'engine',\n",
       "  'front',\n",
       "  'road',\n",
       "  'mile',\n",
       "  'speed',\n",
       "  'good'],\n",
       " ['information',\n",
       "  'list',\n",
       "  'group',\n",
       "  'email',\n",
       "  'internet',\n",
       "  'send',\n",
       "  'service',\n",
       "  'mail',\n",
       "  'address',\n",
       "  'posting'],\n",
       " ['question',\n",
       "  'argument',\n",
       "  'people',\n",
       "  'reason',\n",
       "  'evidence',\n",
       "  'true',\n",
       "  'claim',\n",
       "  'human',\n",
       "  'thing',\n",
       "  'atheist'],\n",
       " ['drive',\n",
       "  'card',\n",
       "  'system',\n",
       "  'disk',\n",
       "  'scsi',\n",
       "  'driver',\n",
       "  'mb',\n",
       "  'problem',\n",
       "  'bit',\n",
       "  'mac'],\n",
       " ['nntppostinghost',\n",
       "  'distribution',\n",
       "  'sale',\n",
       "  'university',\n",
       "  'email',\n",
       "  'price',\n",
       "  'usa',\n",
       "  'keywords',\n",
       "  'im',\n",
       "  'offer'],\n",
       " ['problem',\n",
       "  'year',\n",
       "  'drug',\n",
       "  'food',\n",
       "  'study',\n",
       "  'effect',\n",
       "  'time',\n",
       "  'health',\n",
       "  'medical',\n",
       "  'day'],\n",
       " ['power',\n",
       "  'ground',\n",
       "  'work',\n",
       "  'current',\n",
       "  'water',\n",
       "  'question',\n",
       "  'unit',\n",
       "  'radio',\n",
       "  'wire',\n",
       "  'light'],\n",
       " ['article',\n",
       "  'people',\n",
       "  'opinion',\n",
       "  'dont',\n",
       "  'db',\n",
       "  'make',\n",
       "  'read',\n",
       "  'mine',\n",
       "  'world',\n",
       "  'call'],\n",
       " ['god',\n",
       "  'christian',\n",
       "  'jesus',\n",
       "  'bible',\n",
       "  'life',\n",
       "  'church',\n",
       "  'people',\n",
       "  'faith',\n",
       "  'religion',\n",
       "  'christ']]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\n",
    "# Creating dictionnary from text\n",
    "print(\"Creating Dictionnary...\")\n",
    "dictionary = gensim.corpora.Dictionary(processed_df['processed_text'])\n",
    "# Removing useless words\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Transforming text into bag of words\n",
    "print(\"Creating bag of words...\")\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_df['processed_text']]\n",
    "# Making it human readable\n",
    "# id_words = [[(dictionary[id], count) for id, count in line] for line in bow_corpus]\n",
    "\n",
    "# Getting topic modeler\n",
    "topic_modeler = getTopicExtractor(bow_corpus, dictionary)\n",
    "\n",
    "# Getting The 20 Topics (concatenate)\n",
    "topics = getTopics(topic_modeler, 20)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Doc2Vec Model...\n",
      "Applying Doc2Vec Model on topics\n",
      "Applying Doc2Vec Model on documents\n",
      "Calculating Similarity\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Doc2Vec on raw text data\n",
    "doc2VecModel = trainDoc2VecModel(processed_df['original_text'])\n",
    "\n",
    "# Applying Doc2Vec on Topics\n",
    "print(\"Applying Doc2Vec Model on topics\")\n",
    "topic_vectors = [doc2VecModel.infer_vector(topic) for topic in topics]\n",
    "\n",
    "# Applying Doc2Vec on each document\n",
    "print(\"Applying Doc2Vec Model on documents\")\n",
    "document_vectors = []\n",
    "for doc in processed_df['processed_text']:\n",
    "    document_vectors.append(doc2VecModel.infer_vector(doc))\n",
    "\n",
    "# Calculating Similarity\n",
    "print(\"Calculating Similarity\")\n",
    "similarity = np.empty((len(document_vectors), len(topic_vectors)))\n",
    "for i, document in enumerate(document_vectors):\n",
    "    for j, topic in enumerate(topic_vectors):\n",
    "        similarity[i,j] = euclidean(document, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}