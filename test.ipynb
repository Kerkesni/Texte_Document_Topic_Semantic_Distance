{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MrZanziba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MrZanziba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Loading Data...\n",
      "Pre-processing Data...\n",
      "Creating Dictionnary...\n",
      "Creating bag of words...\n",
      "Loading LDA Mallet Model...\n",
      "Loading Doc2Vec Model...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Used Ressources :\n",
    "- https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel\n",
    "- https://www.kaggle.com/ragnisah/text-data-cleaning-tweets-analysis\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import ast\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "os.environ.update({'MALLET_HOME':r'C:/Users/MrZanziba/Desktop/Cours/Texte/project/other/mallet-2.0.8'}) # UPDATE THIS\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Data Loading\n",
    "def load_data(url):\n",
    "    data = pd.read_csv(url,)\n",
    "    return data\n",
    "\n",
    "# Punctuation removal\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in str(text) if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "# Sub division of strings into arrays of words\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "# Stop words removal \n",
    "def remove_stopwords(text):\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "# Getting the root/base of words\n",
    "def stemming(text):\n",
    "    ps = nltk.PorterStemmer()\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "# Linking similar words to one word\n",
    "def lemmatizer(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "# Text processing\n",
    "def process_dataframe(data):\n",
    "    df  = pd.DataFrame(data[['text']])\n",
    "    df['text_punct'] = df['text'].apply(lambda x: remove_punct(x))\n",
    "    df['text_tokenized'] = df['text_punct'].apply(lambda x: tokenization(x.lower()))\n",
    "    df['text_nonstop'] = df['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "    df['text_stemmed'] = df['text_nonstop'].apply(lambda x: stemming(x))\n",
    "    df['text_lemmatized'] = df['text_nonstop'].apply(lambda x: lemmatizer(x))\n",
    "    df = df.drop(columns=['text_punct', 'text_tokenized', 'text_nonstop', 'text_stemmed'])\n",
    "    df = df.rename(columns={'text':'original_text', 'text_lemmatized':'processed_text'})\n",
    "    return df\n",
    "\n",
    "# Get Dominant Topic for text\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "# Getting Quality Measures of the LDA Model\n",
    "def getPerplexityCoherence(model, corpus, texts, dictionary):\n",
    "    perplexity = model.log_perplexity(corpus)\n",
    "    coherence_model_lda = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence = coherence_model_lda.get_coherence()\n",
    "    return perplexity,coherence\n",
    "\n",
    "# Getting dominant topic for each text\n",
    "def getDominantTopicsForTexts(ldaModel, corpus, texts):\n",
    "    print('Getting Dominant Topics...')\n",
    "    df_topic_sents_keywords = format_topics_sentences(ldaModel=ldaModel, corpus=corpus, texts=texts)\n",
    "    #Formating The Results\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    return df_dominant_topic\n",
    "\n",
    "# Getting Array of Topics\n",
    "def getTopics(ldaModel, n_topics):\n",
    "    topics = []\n",
    "    for index in range(20):\n",
    "        topic = []\n",
    "        for word, n in topic_modeler.show_topic(index):\n",
    "            topic.append(word)\n",
    "        topics.append(topic)\n",
    "    return topics\n",
    "\n",
    "# Training Or loading topic modeler\n",
    "def getTopicExtractor(corpus, dictionary, load=True):\n",
    "    if load:\n",
    "        # Loading Mallet LDA Model\n",
    "        print('Loading LDA Mallet Model...')\n",
    "        temp_file = datapath(\"C:/Users/MrZanziba/Desktop/Cours/Texte/project/model/mallet_model/mallet\") # UPDATE THIS\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet.load(temp_file)\n",
    "        return ldamallet\n",
    "    else:\n",
    "        # Training Mallet LDA Model\n",
    "        print('Training LDA Mallet Model')\n",
    "        mallet_path = 'C:/Users/MrZanziba/Desktop/Cours/Texte/project/other/mallet-2.0.8/bin/mallet' # UPDATE THIS\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow_corpus, num_topics=20, id2word=dictionary)\n",
    "        # Saving The Model\n",
    "        temp_file = datapath(\"C:/Users/MrZanziba/Desktop/Cours/Texte/project/model/mallet_model/mallet\") # UPDATE THIS\n",
    "        ldamallet.save(temp_file)\n",
    "        return ldamallet\n",
    "\n",
    "# Training Doc2Vec Model\n",
    "def trainDoc2VecModel(texts, load=True):\n",
    "    if load:\n",
    "        print('Loading Doc2Vec Model...')\n",
    "        temp_file = datapath(\"C:/Users/MrZanziba/Desktop/Cours/Texte/project/model/doc2vec_model/doc2vec\") # UPDATE THIS\n",
    "        model = Doc2Vec.load(temp_file)\n",
    "        return model\n",
    "    else:\n",
    "        print('Training Doc2Vec Model...')\n",
    "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(texts)]\n",
    "        model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "        temp_file = datapath(\"C:/Users/MrZanziba/Desktop/Cours/Texte/project/model/doc2vec_model/doc2vec\") # UPDATE THIS\n",
    "        model.save(temp_file)\n",
    "        return model\n",
    "\n",
    "# Loading dataset\n",
    "print('Loading Data...')\n",
    "news = load_data('./data/20_newsgroup_train.csv')\n",
    "\n",
    "# Processing dataset\n",
    "print('Pre-processing Data...')\n",
    "processed_df = process_dataframe(news)\n",
    "\n",
    "# Creating dictionnary from text\n",
    "print(\"Creating Dictionnary...\")\n",
    "dictionary = gensim.corpora.Dictionary(processed_df['processed_text'])\n",
    "# Removing useless words\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Transforming text into bag of words\n",
    "print(\"Creating bag of words...\")\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_df['processed_text']]\n",
    "# Making it human readable\n",
    "# id_words = [[(dictionary[id], count) for id, count in line] for line in bow_corpus]\n",
    "\n",
    "# Getting topic modeler\n",
    "topic_modeler = getTopicExtractor(bow_corpus, dictionary)\n",
    "\n",
    "# Getting The 20 Topics (concatenate)\n",
    "topics = getTopics(topic_modeler, 20)\n",
    "\n",
    "# Getting Doc2Vec on raw text data\n",
    "doc2VecModel = trainDoc2VecModel(processed_df['original_text'])\n",
    "\n",
    "# Applying Doc2Vec on Topics\n",
    "topic_vectors = [doc2VecModel.infer_vector(topic) for topic in topics]\n",
    "\n",
    "# Applying Doc2Vec on each document\n",
    "document_vectors = []\n",
    "for doc in processed_df['processed_text']:\n",
    "    document_vectors.append(doc2VecModel.infer_vector(doc))\n",
    "\n",
    "# Calculating Similarity\n",
    "similarity = np.empty((len(document_vectors), len(topic_vectors)))\n",
    "for i, document in enumerate(document_vectors):\n",
    "    for j, topic in enumerate(topic_vectors):\n",
    "        similarity[i,j] = euclidean(document, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}